rnorm(3,0,0.1)
Y = 10 + rnorm(1000,0,0.1)
(Y-mean(Y)*T/(T+1))^2/T
sum((Y-mean(Y)*T/(T+1))^2)/T
a = c(1,2,3)
a
a^2
sum(a^2)
sum((Y-mean(Y))^2)/T
T = 1000
Y = 10 + rnorm(T,0,0.1)
sum((Y-mean(Y)*T/(T+1))^2)/T
sum((Y-mean(Y))^2)/T
T = 100
Y = 10 + rnorm(T,0,0.1)
sum((Y-mean(Y)*T/(T+1))^2)/T
sum((Y-mean(Y))^2)/T
set.seed(1)
T = 100
Y = 10 + rnorm(T,0,0.1)
sum((Y-mean(Y)*T/(T+1))^2)/T
sum((Y-mean(Y))^2)/T
set.seed(1)
T = 100
Y = 10 + rnorm(T,0,0.1)
sum((Y-mean(Y)*T/(T+1))^2)/T
sum((Y-mean(Y))^2)/T
set.seed(1)
T = 100
Y = 10 + rnorm(T,0,0.1)
sum((Y-mean(Y)*T/(T+1))^2)/T
sum((Y-mean(Y))^2)/T
set.seed(1)
T = 1000
set.seed(1)
T = 1000
Y = 10 + rnorm(T,0,0.1)
sum((Y-mean(Y)*T/(T+1))^2)/T
sum((Y-mean(Y))^2)/T
rbinom(10,1,0.5)
a%*%a
sig = 0.1
T=1000
Y = 10 + sig(1-2*rbinom(T,1,.5))
sum((Y-Y%*%Y/sum(Y))^2)/T # MSE for the reverse least squares estimator
sum((Y-mean(Y))^2)/T # MSE for the OLS estimator
sig = 0.1
T=1000
Y = 10 + sig(1-2*rbinom(T,1,.5))
sum((Y-Y%*%Y/sum(Y))^2)/T # MSE for the reverse least squares estimator
sum((Y-mean(Y))^2)/T # MSE for the OLS estimator
sig = 0.1
T=1000
Y = 10 + sig*(1-2*rbinom(T,1,.5))
sum((Y-Y%*%Y/sum(Y))^2)/T # MSE for the reverse least squares estimator
sum((Y-mean(Y))^2)/T # MSE for the OLS estimator
sig = 0.1
T=100
Y = 10 + sig*(1-2*rbinom(T,1,.5))
sum((Y-Y%*%Y/sum(Y))^2)/T # MSE for the reverse least squares estimator
sum((Y-mean(Y))^2)/T # MSE for the OLS estimator
sig = 0.5
T=100
Y = 10 + sig*(1-2*rbinom(T,1,.5))
sum((Y-Y%*%Y/sum(Y))^2)/T # MSE for the reverse least squares estimator
sum((Y-mean(Y))^2)/T # MSE for the OLS estimator
plus10(1)
library(aod)
library(ggplot2)
library(Rcpp)
install.packages("aod")
library(aod)
library(ggplot2)
library(Rcpp)
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
## view the first few rows of the data
head(mydata)
summary(mydata)
sapply(mydata, sd)
mydata$rank <- factor(mydata$rank)#离散型变量
## Model 1: logit regression
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
confint(mylogit)
stats:::vcov
methods("vcov")
stats::vcov
summary(mylogit).glm
summary(mylogit).$cov
aa<-summary(mylogit)
attr(aa)
names(aa)
aa$cov.scaled
aa$cov.unscaled
confint.default(mylogit)
## odds ratios only
exp(coef(mylogit))
## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
## 模型预测1 - predict rank
newdata1 <- with(mydata, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
newdata1$rankP <- predict(mylogit, newdata = newdata1, type = "response")
newdata1
newdata1
## 模型预测2
newdata2 <- with(mydata, data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100), 4),
gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))
View(newdata2)
View(newdata2)
newdata3 <- cbind(newdata2, predict(mylogit, newdata = newdata2, type="link", se=TRUE))
View(newdata3)
View(newdata3)
newdata3 <- cbind(newdata2, predict(mylogit, newdata = newdata2, type="link", se=TRUE))
newdata3
# 预测关联规模
newdata3 <- within(newdata3, {
PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))})
newdata3
head(newdata3)
# 绘制预测概率 - Plot the predicted probablibity
ggplot(newdata3, aes(x = gre, y = PredictedProb)) +
geom_ribbon(aes(ymin = LL, ymax = UL, fill = rank), alpha = .2) +
geom_line(aes(colour = rank), size=1)
# 这两个模型的偏差（即，这个测试的统计量）
with(mylogit, null.deviance - deviance)
# 自由度之差：模型里预测变量的个数
with(mylogit, df.null - df.residual)
# 提取p值
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
# 对数似然
logLik(mylogit)
# reference: http://www.ats.ucla.edu/stat/r/dae/logit.htm
# read data
fdir = "/Users/kungangzhang/Documents/Other/Live_Courses/BitTiger/Full_Stack_Data/class_demo/3-data_analytics/dataset/SH_house_pricing.csv";
data_SH<-read.csv(fdir)
# extraxt training dataset and test dataset
train_ds<-data_SH[1:12,]
test_ds<-data_SH[13:14,]
attach(train_ds)
# plot the data
plot(t,y)
##########################################
## Model1: 时间指数模型
##########################################
View(data_SH)
# 1. Map Y to log coordinate and plot
y1=log(y)
plot(t,y1)
subplot()
# 2. Run Linear Regression
reg=lm(y1~t)
# 3. Print the model result
summary(reg)
# 4. Model Interpret
# 推导过程
ym=y
# intercept
a1 = coefficients(reg)[1] # -3.281e+02
a = exp(a1)
# slope
b1 = coefficients(reg)[2] #1.681e-01
b = b1
y1=a1+b1*t
y1=log(ym)
## 时间指数模型
yy=a*exp(b*t)
# 绘图
plot(t,ym)
lines(t,yy)
# 5. Prediction - 指数方程预测房价
tt=2011:2012
y_rs=a*exp(b*tt)
# 绘图
plot(c(t,tt), c(ym,y_rs))
lines(c(t,tt), c(yy,y_rs))
# Model accurancy
error1<-mean((test_ds$y - y_rs)/test_ds$y)
plot(train_ds)
attach(train_ds)
# 1. calculate the correlation
cor(train_ds)
cor.test(ym,x1)
cor.test(ym,x2)
cor.test(ym,x3)
cor.test(ym,x4)
cor.test(ym,x5)
cor.test(ym,x6)
cor.test(ym,x7)
# 2. Multi-var linear regression
# Model 1: 7 vars
reg1=lm(ym~x1+x2+x3+x4+x5+x6+x7)
summary(reg1)
# Model 2: 6 vars
reg2=lm(ym~x1+x2+x3+x4+x6+x7)
summary(reg2)
# Model 3: 5 vars
reg3=lm(ym~x1+x2+x3+x6+x7)
summary(reg3)
# 3. Model Interpretation
bb=coefficients(reg3)
b0=bb[[1]]
b1=bb[[2]]
b2=bb[[3]]
b3=bb[[4]]
b6=bb[[5]]
b7=bb[[6]]
attach(test_ds)
# model prediction for 2010:2011
yy2=b0+b1*x1+b2*x2+b3*x3+b6*x6+b7*x7
# 4. Prediction
# predict year: 2011:2012
# result: 19254 24534
y2_rs=b0+b1*x1+b2*x2+b3*x3+b6*x6+b7*x7
# 5. Model accurancy
error2<-mean((test_ds$y - y2_rs)/test_ds$y)
install.packages(c('rzmq','repr','IRkernel','IRdisplay'), repos = 'http://irkernel.github.io/', type = 'source')
IRkernel::installspec()
quit()
df_std[-1]
setwd("/Users/kungangzhang/Documents/OneDrive/Northwestern/Study/Courses/MSiA-420-0/HW2")
rm(list = ls())
require(gdata)
set.seed(111)
#Prob 1)
##(a)Fit a linear model and discuss the predictive power.
##Answer: First I take log transform to the cost, the response variable, and then fit the model with all predictors unchanged. The $R^2$ is $0.5831$. Then, I tried to standardize everything and the $R^2$ is $0.5527$. I saw some of predictors also have skewed distribution or long tail problem, so that I try log transform (or some special log transform depending on whether it is left-skewed and right-skewed), and the histograms look more symmetric. For the rest of predictors, I just let them be. The $R^2$ increases to $0.658$. Generally, those predictors significant before transform are also significant afterwards.
##The histogram of each columns
df<-read.xls("./HW2_data.xls",sheet=1,header=TRUE)
par(mfrow=c(3,3))
for (i in seq(2,10)) hist(df[[i]],breaks=30,xlab=names(df)[i])
mod1<-lm(log10(cost)~.,data = df[-1])
summary(mod1)
##Also, I tried to standardize each variable to see effect.
df_std<-df
df_std$cost <- log10(df_std$cost)
df_std[2:10]<-sapply(df[2:10], function(x) (x-mean(x))/sd(x))
mod2<-lm(cost~.,data=df_std[-1])
summary(mod2)
df_std[-1]
dd<-df_std[-1]
summary(mod1)
summary(mod2)
summary(df_std)
sapply(df_std,sd)
summary(mod1)
summary(mod2)
summary(mod1)
summary(mod2)
summary(mod1)
summary(mod2)
summary(mod1)
svd(df[-1])
dfsvd<-svd(df[-1])
summary(dfsvd)
dfsvd$d
df$gend <- as.factor(df$gend)
for (i in seq(2,10)) hist(df[[i]],breaks=30,xlab=names(df)[i])
for (i in seq(2,10)) hist(df[[i]],breaks=30,xlab=names(df)[i])
df0<-read.xls("./HW2_data.xls",sheet=1,header=TRUE)
par(mfrow=c(3,3))
df0<-df
df$gend <- as.factor(df$gend)
for (i in seq(2,10)) hist(df[[i]],breaks=30,xlab=names(df0)[i])
df0<-read.xls("./HW2_data.xls",sheet=1,header=TRUE)
par(mfrow=c(3,3))
df<-df0
df$gend <- as.factor(df$gend)
for (i in seq(2,10)) hist(df[[i]],breaks=30,xlab=names(df0)[i])
for (i in seq(2,10)) hist(df0[[i]],breaks=30,xlab=names(df0)[i])
df$gend <- as.factor(df$gend)
for (i in seq(2,10)) hist(df0[[i]],breaks=30,xlab=names(df0)[i])
setwd("/Users/kungangzhang/Documents/OneDrive/Northwestern/Study/Courses/MSiA-420-0/HW2")
rm(list = ls())
require(gdata)
set.seed(111)
#Prob 1)
##(a)Fit a linear model and discuss the predictive power.
##Answer: First I take log transform to the cost, the response variable, and then fit the model with all predictors unchanged. The $R^2$ is $0.5831$. Then, I tried to standardize everything and the $R^2$ is $0.5527$. I saw some of predictors also have skewed distribution or long tail problem, so that I try log transform (or some special log transform depending on whether it is left-skewed and right-skewed), and the histograms look more symmetric. For the rest of predictors, I just let them be. The $R^2$ increases to $0.658$. Generally, those predictors significant before transform are also significant afterwards.
##The histogram of each columns
df0<-read.xls("./HW2_data.xls",sheet=1,header=TRUE)
par(mfrow=c(3,3))
df<-df0
df$gend <- as.factor(df$gend)
for (i in seq(2,10)) hist(df0[[i]],breaks=30,xlab=names(df0)[i])
mod1<-lm(log10(cost)~.,data = df[-1])
summary(mod1)
df_std<-df
df_std$cost <- log10(df_std$cost)
df_std[c(2,3,5:10)]<-sapply(df[c(2,3,5:10)], function(x) (x-mean(x))/sd(x))
mod2<-lm(cost~.,data = df_std[-1])
summary(mod2)
df$cost <- log10(df$cost)
mod1<-lm(cost~.,data = df[-1])
summary(mod1)
mod1$model
model <- data.frame(mod1$model,"1"=matrix(1,nrow(df),1))
t1 <- model[-1]%*% solve(t(model[-1]) %*% model[-1]) %*% t(model[-1]) *model[1]
t(model[-1])
XX<-data.matrix(model[-1])
View(XX)
